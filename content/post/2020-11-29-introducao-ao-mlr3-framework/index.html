---
title: Introdução ao framework {mlr3}
author: Alberson Miranda
date: '2020-12-27'
slug: [introducao-ao-mlr3]
series: mlr3
categories:
  - Machine Learning
tags:
  - mlr3
  - R
description: Uma introdução ao framework de machine learning mlr3, um pacote de R.
featured: yes
draft: false
featureImage: img/mlr3verse.svg
thumbnail: images/mlr3.png
shareImage: images/posts/2020-12-20 - intro-mlr3.jpeg
figurePositionShow: yes
bibliography: [bib.bib]
link-citations: true
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>Esse é o primeiro post de uma série que irá tratar sobre o ecossistema {mlr3} <span class="citation">(<a href="#ref-mlr3" role="doc-biblioref">Lang et al. 2019</a>)</span>. Ele é mais completo e também muito mais complexo do que seu predecessor, o {mlr}, que teve sua versão inicial publicada no CRAN em 2013. O ecossistema permite um framework agnóstico (i.e. não depende dos algoritmos escolhidos), extensível e orientado a objeto, e, atualmente, permite vários tipos de tarefas, como classificação, regressão, análise de sobrevivência, forecasting, clustering, dentre outros. O {mlr3} tem diversas vantagens que o faz, IMHO, o <em>framework</em> mais completo para se trabalhar <em>machine learning</em> em R <span class="citation">(<a href="#ref-R" role="doc-biblioref">R Core Team 2020</a>)</span> e elas ficarão claras ao longo dos próximos posts.</p>
<div id="introdução" class="section level1">
<h1>INTRODUÇÃO</h1>
<p>O workflow padrão de um projeto de machine learning consiste em:</p>
<ol style="list-style-type: decimal">
<li>dividir sua amostra em treino e teste (<em>split</em>);</li>
<li>escolher o algoritmo<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> apropriado para o tipo de tarefa;</li>
<li>passar a amostra de treino ao algoritmo para criar um modelo do relacionamento entre a variável de resposta (<em>output features</em>) e as explicativas (<em>input features</em>);</li>
<li>passar os dados de testes ao modelo treinado para produzir predições;</li>
<li>comparar as predições com os dados da amostra;</li>
<li>mensurar a performance do modelo através de medidas de acurácia estabelecidas.</li>
</ol>
<p><img src="img/basics.svg" /></p>
<p>O processo de repetir esse workflow várias vezes, separando a amostra treino em várias partes diferentes e usando outras como <em>fake test samples</em> é chamado de <em>resampling</em>, um processo vital para a etapa de calibragem e para evitar o <em>overfitting</em>.</p>
<p>Dependendo dos dados, do tipo de tarefa e algoritmo escolhido, podem ser necessários vários filtros, como normalização, <em>feature selection</em> e tratamento de <em>outliers</em> ou dados faltantes. Para esses casos, o {mlr3} tem novas soluções que se destacam com muita vantagem em relação não só ao seu predecessor {mlr} como também a outros <em>frameworks</em> de <em>machine learning</em> em R, como o {caret} e o {tidymodels}.</p>
<div id="características-de-design" class="section level2">
<h2>CARACTERÍSTICAS DE DESIGN</h2>
<p>Alguns princípios gerais que norteiam o desenvolvimento do pacote e afetam muito seu uso são:</p>
<ul>
<li><p>Foco no <em>backend</em>. A maioria dos pacotes do ecossistema tem objetivo de processar e transformar dados, aplicar algoritmos e computar resultados. Visualizações são providenciadas em pacotes externos;</p></li>
<li><p>Adoção da classe R6 <span class="citation">(<a href="#ref-R-R6" role="doc-biblioref">Chang 2020</a>)</span> para design orientado a objeto, <em>modify-in-place</em> e semântica de referência (falaremos um pouco sobre esses conceitos adiante);</p></li>
<li><p>Adoção do {data.table} <span class="citation">(<a href="#ref-R-data.table" role="doc-biblioref">Dowle and Srinivasan 2020</a>)</span> para manipulações de <em>data frames</em>. A combinação {R6} + {data.table} torna a performance um dos pontos fortes do ecossistema.</p></li>
<li><p>Baixa dependência. Entretanto, os algortimos não são implementados no ecossistema, como no scikit-learn em Python. Para executar o XGBoost <span class="citation">(<a href="#ref-xgboost" role="doc-biblioref">Chen et al. 2020</a>)</span>, por exemplo, deve-se ter instalado o pacote que o implementa.</p></li>
</ul>
</div>
<div id="fora-do-escopo" class="section level2">
<h2>FORA DO ESCOPO</h2>
<p>Como se trata de uma introdução, as etapas de <em>tunning</em> e <em>resampling</em>, assim como funcionalidades como os <em>pipelines</em>, serão abordadas em postagens futuras. Neste post trataremos apenas os conceitos básicos do <em>workflow</em>.</p>
</div>
</div>
<div id="direto-ao-ponto" class="section level1">
<h1>DIRETO AO PONTO</h1>
<p>Para conhecer as funcionalidades básicas do pacote, usaremos um dos <em>datasets</em> inclusos no R, o <code>swiss</code>. Esse <em>dataset</em> consiste na medição padronizada da fertilidade e indicadores socioeconômicos de 47 províncias da Suíça em 1888.</p>
{{% highlight "r" %}}

# criando dataframe
data = swiss

# overview
skimr::skim(data)

{{% /highlight %}}
<table>
<caption><span id="tab:data">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">data</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">47</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">6</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">6</td>
</tr>
<tr class="odd">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Fertility</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">70.14</td>
<td align="right">12.49</td>
<td align="right">35.00</td>
<td align="right">64.70</td>
<td align="right">70.40</td>
<td align="right">78.45</td>
<td align="right">92.5</td>
<td align="left">▂▂▇▇▅</td>
</tr>
<tr class="even">
<td align="left">Agriculture</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">50.66</td>
<td align="right">22.71</td>
<td align="right">1.20</td>
<td align="right">35.90</td>
<td align="right">54.10</td>
<td align="right">67.65</td>
<td align="right">89.7</td>
<td align="left">▃▃▆▇▅</td>
</tr>
<tr class="odd">
<td align="left">Examination</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">16.49</td>
<td align="right">7.98</td>
<td align="right">3.00</td>
<td align="right">12.00</td>
<td align="right">16.00</td>
<td align="right">22.00</td>
<td align="right">37.0</td>
<td align="left">▅▇▆▂▂</td>
</tr>
<tr class="even">
<td align="left">Education</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">10.98</td>
<td align="right">9.62</td>
<td align="right">1.00</td>
<td align="right">6.00</td>
<td align="right">8.00</td>
<td align="right">12.00</td>
<td align="right">53.0</td>
<td align="left">▇▃▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">Catholic</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">41.14</td>
<td align="right">41.70</td>
<td align="right">2.15</td>
<td align="right">5.20</td>
<td align="right">15.14</td>
<td align="right">93.12</td>
<td align="right">100.0</td>
<td align="left">▇▁▁▁▅</td>
</tr>
<tr class="even">
<td align="left">Infant.Mortality</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">19.94</td>
<td align="right">2.91</td>
<td align="right">10.80</td>
<td align="right">18.15</td>
<td align="right">20.00</td>
<td align="right">21.70</td>
<td align="right">26.6</td>
<td align="left">▁▂▇▆▂</td>
</tr>
</tbody>
</table>
<p>Dentre as vari]]áveis disponíveis, podemos escolher modelar a mortalidade infantil <code>Infant.Mortality</code> baseada nas demais features, que são:</p>
<ul>
<li><code>Fertility</code>: Medida de fertilidade. Assim como a mortalidade infantil, está escalonada entre 0-100.</li>
<li><code>Agriculture</code>: Percentual de homens envolvidos em agricultura como ocupação.</li>
<li><code>Examination</code>: Percentual de alistados bem avaliados nos exames do exército.</li>
<li><code>Education</code>: Percentual dos alistados com educação superior ao primário.</li>
<li><code>Catholic</code>: Percentual de católicos (em oposto a protestantes).</li>
</ul>
<p>O <em>workflow</em> começa pela criação da <code>task</code>, que é um objeto que contém os dados e informações sobre a tarefa a ser executada, como a variável de resposta<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> e as demais <em>features</em>, além de seus tipos. Como queremos realizar predições para uma variável numérica contínua, é uma tarefa de regressão.</p>
{{% highlight "r" %}}

# importando pacote
library(mlr3verse)

# criando task
task_swiss = TaskRegr$new(
  id = "swiss",
  backend = data,
  target = "Infant.Mortality"
)

{{% /highlight %}}
<p>Notou algo estranho? Como o {mlr3} trabalha com a classe R6, seu manuseio é mais parecido com outras linguagens orientadas a objeto, como o Python. Essa classe tem duas propriedades especiais:</p>
<ul>
<li><p>Métodos pertencem a objetos e são chamados na forma <code>objeto$metodo()</code> e não como funções genéricas como <code>foo()</code>. Esse é o paradigma OOP (programação orientada à objetos). No caso acima, não há uma função para criar uma <em>task</em> como <code>task_regr_new()</code>, mas um método <code>new()</code> associado ao objeto <code>TaskRegr</code>;</p></li>
<li><p>Os objetos da classe R6 são mutáveis, ou seja, eles são modificados no local (<em>modify-in-place</em>) e, portanto, têm semântica de referência. Isso significa que eles não são copiados a cada modificação, como os <em>data frames</em> comuns (classe S3), o que é um fator de alocação de memória e, consequentemente, lentidão.</p></li>
</ul>
<p>O efeito colateral é que isso não é muito familiar para pessoas que não conhecem outras linguagens de programação além do R e no início pode parecer pouco natural e confuso.</p>
<p>Criado o objeto, podemos acessá-lo para verificar e visualizar as informações ali contidas:</p>
{{% highlight "r" %}}

# verificando
task_swiss

# visualizando
autoplot(task_swiss, type = "pairs")

{{% /highlight %}}
<pre><code>## &lt;TaskRegr:swiss&gt; (47 x 6)
## * Target: Infant.Mortality
## * Properties: -
## * Features (5):
##   - dbl (3): Agriculture, Catholic, Fertility
##   - int (2): Education, Examination</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/task_cont-1.png" width="672" /></p>
<p>Podemos ver que apenas a fertilidade é linearmente correlacionada com a mortalidade infantil — quanto maior a fertilidade, maior a mortalidade —, e podemos esperar que tenha maior peso nas predições. As demais variáveis não apresentam correlação linear significativa com a variável de resposta. Entretanto, apresentam correlação moderada ou forte entre si, mas não a ponto de apresentar colinearidade, o que demandaria tratamento.</p>
<p>Agora selecionamos o algoritmo<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> que será usado para treinar o modelo. Escolhi aqui o <em>XGBoost</em>. A lista completa pode ser acessada <a href="https://mlr3extralearners.mlr-org.com/articles/learners/learner_status.html">por essa lista estática</a>, <a href="https://mlr3extralearners.mlr-org.com/articles/learners/list_learners.html">por essa lista dinâmica</a> ou pela função <code>mlr3extralearners::list_mlr3learners()</code>. Como mencionado anteriormente, os algoritmos não são implementados pelo ecossistema do {mlr3} e nas listas citadas constam os pacotes onde os algoritmos foram implementados e que devem ser baixados para o seu uso.</p>
{{% highlight "r" %}}

# definindo o learner
l_xgboost = lrn("regr.xgboost")

# checando
l_xgboost

{{% /highlight %}}
<pre><code>## &lt;LearnerRegrXgboost:regr.xgboost&gt;
## * Model: -
## * Parameters: nrounds=1, nthread=1, verbose=0
## * Packages: xgboost
## * Predict Type: response
## * Feature types: logical, integer, numeric
## * Properties: importance, missings, weights</code></pre>
<p>Vamos entender o que o objeto <code>l_xgboost</code> nos diz.</p>
<ol style="list-style-type: decimal">
<li><strong>Model</strong>: Vazio, pois ainda não há um modelo treinado;</li>
<li><strong>Parameters</strong>: Os hiperparâmetros a serem escolhidos e tunados para performance do modelo;</li>
<li><strong>Packages</strong>: O pacote onde o algorítmo foi implementado e de onde será importado pelo {mlr3};</li>
<li><strong>Predict Type</strong>: Se <code>response</code> a predição é retornada como 0 ou 1, no caso de classificação, ou num valor para variável de resposta, no caso de regressão — neste caso, será a mortalidade infantil escalonada no intervalo [1, 100]. Se “prob,” para classificação, a predição retorna a probabilidade entre 0 e 1;</li>
<li><strong>Feature Type</strong>: Os tipos de variáveis que o algoritmo é capaz de manipular. No caso do <em>XGBoost</em>, por exemplo, apenas variáveis numéricas podem ser utilizadas. Isso quer dizer que os fatores devem ser convertidos em valores binários (i.e. 0 ou 1), ou seja, deve-se tornar a matriz esparsa — no caso de um fator <code>sexo</code>, por exemplo, na fase de pre-processamento seriam criadas as colunas <code>sexo.masculino</code>, com valores de 1 ou 0, e <code>sexo.feminino</code>, também com 1 ou 0;</li>
<li><strong>Properties</strong>: Propriedades e capacidades adicionais do algoritmo. Neste caso, o <strong>XGBoost</strong> possui a capacidade de computar e retornar os valores da importância das <em>features</em> para o modelo; a capacidade de trabalhar com dados faltantes (<em>missings</em>); a capacidade de computar e retornar os pesos associados às <em>features</em>.</li>
</ol>
<p>Como pode ver em <em>parameters</em>, não há nenhum hiperparâmetro configurado. Podemos acessá-los da seguinte maneira:</p>
{{% highlight "r" %}}

# acessando hiperparâmetros
head(as.data.table(l_xgboost$param_set))

{{% /highlight %}}
<pre><code>##                  id    class lower upper               levels nlevels
## 1:          booster ParamFct    NA    NA gbtree,gblinear,dart       3
## 2:        watchlist ParamUty    NA    NA                          Inf
## 3:              eta ParamDbl     0     1                          Inf
## 4:            gamma ParamDbl     0   Inf                          Inf
## 5:        max_depth ParamInt     0   Inf                          Inf
## 6: min_child_weight ParamDbl     0   Inf                          Inf
##    is_bounded special_vals default storage_type  tags
## 1:       TRUE    &lt;list[0]&gt;  gbtree    character train
## 2:      FALSE    &lt;list[0]&gt;                 list train
## 3:       TRUE    &lt;list[0]&gt;     0.3      numeric train
## 4:      FALSE    &lt;list[0]&gt;       0      numeric train
## 5:      FALSE    &lt;list[0]&gt;       6      integer train
## 6:      FALSE    &lt;list[0]&gt;       1      numeric train</code></pre>
<p>Como o <em>tunning</em> de hiperparâmetros não é o assunto, vamos apenas configurar algumas coisas básicas para demonstrar como essas informações são acessadas e modificadas. O método para isso é o <code>param_set$values</code>:</p>
{{% highlight "r" %}}

# hiperparâmetros
l_xgboost$param_set$values = list(
  # mandando o algoritmo parar depois de 10 iterações sem melhora no score
  early_stopping_rounds = 10,
  # mandando o algoritmo treinar mais lentamente
  eta = 0.1,
  # limitando profundidade da árvore
  max_depth = 5,
  # quantidade máxima de iterações
  nrounds = 100
)

# verificando
l_xgboost

{{% /highlight %}}
<pre><code>## &lt;LearnerRegrXgboost:regr.xgboost&gt;
## * Model: -
## * Parameters: early_stopping_rounds=10, eta=0.1, max_depth=5,
##   nrounds=100
## * Packages: xgboost
## * Predict Type: response
## * Feature types: logical, integer, numeric
## * Properties: importance, missings, weights</code></pre>
</div>
<div id="treino-e-predição" class="section level1">
<h1>TREINO E PREDIÇÃO</h1>
<p>As próximas etapas são o treino e a predição — trataremos de <em>tunning</em> e <em>resampling</em> nos próximos posts. Primeiramente, o <em>split</em> do dataset em treino e teste. Para isso, usaremos a função <code>sample()</code> em dois métodos do objeto <code>task_swiss</code>, o <code>row_ids</code> e <code>nrow</code>. O primeiro enumera os índices de cada linha:</p>
{{% highlight "r" %}}

# método row_ids
task_swiss$row_ids

{{% /highlight %}}
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
## [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47</code></pre>
<p>Enquanto o segundo retorna a quantidade de linhas do dataset:</p>
{{% highlight "r" %}}

# método row_ids
task_swiss$nrow

{{% /highlight %}}
<pre><code>## [1] 47</code></pre>
<p>Assim, podemos selecionar os índice do dataset em duas amostras aleatórias:</p>
{{% highlight "r" %}}

# garantindo reprodutibilidade
set.seed(1)

# índices para amostra treino
train_set = sample(task_swiss$row_ids, 0.7 * task_swiss$nrow)

# índices para amostra teste
test_set = setdiff(task_swiss$row_ids, train_set)

# verificando
head(train_set)

{{% /highlight %}}
<pre><code>## [1]  4 39  1 34 23 14</code></pre>
<p>Com os índices selecionados, podemos realizar nosso treino apenas nos 70% escolhidos aleatoriamente da amostra, sem copiar os dados e alocar memória desnecessariamente:</p>
{{% highlight "r" %}}

# treino
l_xgboost$train(task_swiss, row_ids = train_set)

# verificando
l_xgboost$model

{{% /highlight %}}
<pre><code>## [10:21:01] WARNING: amalgamation/../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.
## [1]  train-rmse:17.686886 
## Will train until train_rmse hasn&#39;t improved in 10 rounds.
## 
## [2]  train-rmse:16.014399 
## [3]  train-rmse:14.506416 
## [4]  train-rmse:13.147319 
## [5]  train-rmse:11.924172 
## [6]  train-rmse:10.822335 
## [7]  train-rmse:9.832130 
## [8]  train-rmse:8.940679 
## [9]  train-rmse:8.136598 
## [10] train-rmse:7.413037 
## [11] train-rmse:6.760063 
## [12] train-rmse:6.173677 
## [13] train-rmse:5.647888 
## [14] train-rmse:5.177068 
## [15] train-rmse:4.751254 
## [16] train-rmse:4.367561 
## [17] train-rmse:4.022047 
## [18] train-rmse:3.705540 
## [19] train-rmse:3.421478 
## [20] train-rmse:3.165364 
## [21] train-rmse:2.923110 
## [22] train-rmse:2.709535 
## [23] train-rmse:2.511684 
## [24] train-rmse:2.330560 
## [25] train-rmse:2.162180 
## [26] train-rmse:2.002988 
## [27] train-rmse:1.861695 
## [28] train-rmse:1.730979 
## [29] train-rmse:1.614930 
## [30] train-rmse:1.510077 
## [31] train-rmse:1.405107 
## [32] train-rmse:1.316820 
## [33] train-rmse:1.227179 
## [34] train-rmse:1.148336 
## [35] train-rmse:1.074920 
## [36] train-rmse:1.004666 
## [37] train-rmse:0.940519 
## [38] train-rmse:0.886334 
## [39] train-rmse:0.832195 
## [40] train-rmse:0.782907 
## [41] train-rmse:0.737985 
## [42] train-rmse:0.695803 
## [43] train-rmse:0.658468 
## [44] train-rmse:0.620991 
## [45] train-rmse:0.584939 
## [46] train-rmse:0.551442 
## [47] train-rmse:0.520346 
## [48] train-rmse:0.491411 
## [49] train-rmse:0.464267 
## [50] train-rmse:0.438681 
## [51] train-rmse:0.414958 
## [52] train-rmse:0.391905 
## [53] train-rmse:0.369801 
## [54] train-rmse:0.350035 
## [55] train-rmse:0.330991 
## [56] train-rmse:0.313710 
## [57] train-rmse:0.296662 
## [58] train-rmse:0.280962 
## [59] train-rmse:0.266252 
## [60] train-rmse:0.253146 
## [61] train-rmse:0.240702 
## [62] train-rmse:0.229117 
## [63] train-rmse:0.217732 
## [64] train-rmse:0.205889 
## [65] train-rmse:0.195538 
## [66] train-rmse:0.186375 
## [67] train-rmse:0.176608 
## [68] train-rmse:0.168004 
## [69] train-rmse:0.160256 
## [70] train-rmse:0.151921 
## [71] train-rmse:0.144080 
## [72] train-rmse:0.136428 
## [73] train-rmse:0.130172 
## [74] train-rmse:0.123807 
## [75] train-rmse:0.117861 
## [76] train-rmse:0.112243 
## [77] train-rmse:0.106818 
## [78] train-rmse:0.101794 
## [79] train-rmse:0.096382 
## [80] train-rmse:0.091913 
## [81] train-rmse:0.087061 
## [82] train-rmse:0.082908 
## [83] train-rmse:0.079150 
## [84] train-rmse:0.075045 
## [85] train-rmse:0.071658 
## [86] train-rmse:0.067890 
## [87] train-rmse:0.064352 
## [88] train-rmse:0.061335 
## [89] train-rmse:0.058158 
## [90] train-rmse:0.055464 
## [91] train-rmse:0.052609 
## [92] train-rmse:0.049872 
## [93] train-rmse:0.047326 
## [94] train-rmse:0.045157 
## [95] train-rmse:0.042823 
## [96] train-rmse:0.040716 
## [97] train-rmse:0.038909 
## [98] train-rmse:0.036942 
## [99] train-rmse:0.035096 
## [100]    train-rmse:0.033438</code></pre>
<pre><code>## ##### xgb.Booster
## raw: 156.2 Kb 
## call:
##   xgboost::xgb.train(data = data, nrounds = 100L, watchlist = list(
##     train = &lt;pointer: 0x000000001ca3f8a0&gt;), early_stopping_rounds = 10L, 
##     eta = 0.1, max_depth = 5L, objective = &quot;reg:linear&quot;)
## params (as set within xgb.train):
##   eta = &quot;0.1&quot;, max_depth = &quot;5&quot;, objective = &quot;reg:linear&quot;, validate_parameters = &quot;TRUE&quot;
## xgb.attributes:
##   best_iteration, best_msg, best_ntreelimit, best_score, niter
## callbacks:
##   cb.print.evaluation(period = print_every_n)
##   cb.evaluation.log()
##   cb.early.stop(stopping_rounds = early_stopping_rounds, maximize = maximize, 
##     verbose = verbose)
## # of features: 5 
## niter: 100
## best_iteration : 100 
## best_ntreelimit : 100 
## best_score : 0.033438 
## best_msg : [100] train-rmse:0.033438 
## nfeatures : 5 
## evaluation_log:
##     iter train_rmse
##        1  17.686886
##        2  16.014399
## ---                
##       99   0.035096
##      100   0.033438</code></pre>
<p>Como podemos ver, na primeira iteração o modelo obteve <em>rmse</em><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. de 17.7, o que é alto considerando a escala [1-100] da mortalidade infantil. Ao longo do treino, o erro foi reduzido até 0.03, o que não significa que sua performance permaneça nesse nível quando extrapolado para a amostra teste ou novos dados, mas é um bom sinal. O esperado é que a performance real do modelo, após ser aplicado à amostra teste, fique entre a iteração inicial e final. Se ficar <em>melhor</em> do que a performance do teste, alguma coisa certamente está errada.</p>
<p>Vamos verificar qual a performance real após realizarmos as predições na amostra teste. Primeiro, passamos os índices de teste ao objeto do <em>learner</em> com o modelo e chamamos o método <code>predict()</code> para obter as predições.</p>
{{% highlight "r" %}}

# predições
preds = l_xgboost$predict(task_swiss, row_ids = test_set)

# verificando
preds

{{% /highlight %}}
<pre><code>## &lt;PredictionRegr&gt; for 15 observations:
##     row_ids truth response
##           8  24.9 20.69299
##          11  24.5 20.65381
##          13  19.1 15.63718
## ---                       
##          40  20.5 19.55373
##          43  20.0 21.27585
##          47  19.3 21.82050</code></pre>
<p>No objeto com as predições, são armazenados tanto os valores preditos pelo modelo <code>response</code> quanto os valores da amostra <code>truth</code>. Esses valores então podem ser comparados para calcular a acurácia do modelo através do método <code>score()</code>:</p>
{{% highlight "r" %}}

# acurácia
preds$score(list(
  msr("regr.rmse"),
  msr("regr.mae")
))

# visualizando
autoplot(preds)

{{% /highlight %}}
<pre><code>## regr.rmse  regr.mae 
##  2.587082  2.214177</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/accuracy-1.png" width="672" /></p>
<p>A <em>rmse</em> do modelo na amostra teste ficou em apenas 2.59 unidades, o que é uma performance muito boa!</p>
</div>
<div id="interpretação" class="section level1">
<h1>INTERPRETAÇÃO</h1>
<p>Como o XGBoost possui a propriedade de <em>feature importance</em>, podemos extraí-la com o método <code>importance()</code>:</p>
{{% highlight "r" %}}

# feature importance
l_xgboost$importance()

# visualizando
barplot(l_xgboost$importance())

{{% /highlight %}}
<pre><code>##    Catholic Agriculture   Fertility Examination   Education 
##  0.39696784  0.24723198  0.20219343  0.10340228  0.05020448</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/importance-1.png" width="672" /></p>
<p>Entretanto, somente a importância não nos descreve o relacionamento da <em>feature</em> com a variável de resposta, nem mesmo sua direção, sendo uma medida muito pobre de interpretação. Discutiremos técnicas de interpretação em outras postagens.</p>
</div>
<div id="citação" class="section level1">
<h1>CITAÇÃO</h1>
<p>{{% citacao %}}</p>
</div>
<div id="referências" class="section level1 unnumbered">
<h1>REFERÊNCIAS</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-R-R6" class="csl-entry">
Chang, Winston. 2020. <em>R6: Encapsulated Classes with Reference Semantics</em>. <a href="https://CRAN.R-project.org/package=R6">https://CRAN.R-project.org/package=R6</a>.
</div>
<div id="ref-xgboost" class="csl-entry">
Chen, Tianqi, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. 2020. <em>Xgboost: Extreme Gradient Boosting</em>. <a href="https://CRAN.R-project.org/package=xgboost">https://CRAN.R-project.org/package=xgboost</a>.
</div>
<div id="ref-R-data.table" class="csl-entry">
Dowle, Matt, and Arun Srinivasan. 2020. <em>Data.table: Extension of ‘Data.frame‘</em>. <a href="https://CRAN.R-project.org/package=data.table">https://CRAN.R-project.org/package=data.table</a>.
</div>
<div id="ref-mlr3" class="csl-entry">
Lang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. <span>“<span class="nocase">mlr3</span>: A Modern Object-Oriented Machine Learning Framework in <span>R</span>.”</span> <em>Journal of Open Source Software</em>, December. <a href="https://doi.org/10.21105/joss.01903">https://doi.org/10.21105/joss.01903</a>.
</div>
<div id="ref-R" class="csl-entry">
R Core Team. 2020. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>No {mlr3} é chamado de <em>learner</em>.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Também chamada de <em>output feature</em> ou <em>label</em>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Aqui trabalharemos apenas com um, mas em posts futuros utilizaremos de diversas formas — pipelines com diferentes <em>features</em>, <em>stacking</em> etc.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Raiz do Erro Médio Quadrático<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
